{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM75MRgocy7/YgFuEOIWkdv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Natural-language-processing/blob/main/Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF6zW3gnDvfA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "W_om6ncoEdax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Portion is intended to test all the dependencies that are needed for the rest of the notebooks to run properly. If everything is tested correctly you should see a \"You are all set\" message when running the code below"
      ],
      "metadata": {
        "id": "IrdA4T7iEi1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import ngrams\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from pprint import pprint\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from nltk.corpus import reuters\n",
        "from operator import itemgetter"
      ],
      "metadata": {
        "id": "7WYqdtPVEueW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHrl36fSE72Z",
        "outputId": "7de6accf-dc13-404d-ccdb-bff5e9b4af07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test that verifies that the imports don't fail and that we have the correct files from nltk downloaded\n",
        "reuters.fileids()\n",
        "stopwords.words('english')\n",
        "word_tokenize('This is just a test')\n",
        "\n",
        "print('You are all set')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp7awaDVE25Y",
        "outputId": "25536eb0-879b-46c1-a99a-e17763bd6e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are all set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1- Text Classification**"
      ],
      "metadata": {
        "id": "tpEx53qfD2Mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1-Binary text classification**"
      ],
      "metadata": {
        "id": "S-BPs6llHZgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will address the binary problem of detecting Sport related documents vs any other type of documents. In order to do this we will create an artificial (and very small collection).\n",
        "\n",
        "Define a set of labelled documents that will be our training dataset. These are the documents the classifier will learn from in order to categorise future unseen documents\n",
        "\n",
        "Define a set of labelled documents that will be our testing dataset. These will be the \"unseen\" documents that the classifier will predict (without having being trained with them)\n",
        "\n",
        "Represent our training and testing documents\n",
        "\n",
        "Train the classifier based on the training data\n",
        "\n",
        "Predict the labels for the testing documents"
      ],
      "metadata": {
        "id": "Pz1m1RJUHg3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.1.1-Support Vector Machine (SVM)**"
      ],
      "metadata": {
        "id": "JJXnHE1aHxP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC"
      ],
      "metadata": {
        "id": "z4f61c9bHapn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test data. Both the full documents and their labels (\"Sports\" vs \"Non Sports\")\n",
        "train_data = ['Football: a great sport', 'The referee has been very bad this season', 'Our team scored 5 goals', 'I love tennis',\n",
        "              'Politics is in decline in the UK', 'Brexit means Brexit', 'The parlament wants to create new legislation',\n",
        "              'I so want to travel the world']\n",
        "train_labels = [\"Sports\",\"Sports\",\"Sports\",\"Sports\", \"Non Sports\", \"Non Sports\", \"Non Sports\", \"Non Sports\"]\n",
        "\n",
        "test_data = ['Swimming is a great sport',\n",
        "             'A lot of policy changes will happen after Brexit',\n",
        "             'The table tennis team will travel to the UK soon for the European Championship']\n",
        "test_labels = [\"Sports\",\"Non Sports\",\"Sports\"]"
      ],
      "metadata": {
        "id": "XN7EkSTNH_pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Representation of the data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
        "vectorised_test_data = vectorizer.transform(test_data)\n",
        "\n",
        "# Train the classifier given the training data\n",
        "classifier = LinearSVC()\n",
        "classifier.fit(vectorised_train_data, train_labels)\n",
        "\n",
        "# Predict the labels for the test documents (not used for training)\n",
        "print(classifier.predict(vectorised_test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEsBpwm-IHQk",
        "outputId": "8ee3813e-ce7b-49a0-af2a-2611df24395a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sports' 'Non Sports' 'Non Sports']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, the third case is wrongly classified. Why do you think that might be?\n",
        "\n",
        "Matching problems (e.g., \"car\" is different than \"Cars\")\n",
        "\n",
        "Cases never seen before (e.g., the classifier has never seen the word \"table\")\n",
        "\n",
        "\"Spurious\" correlations and bias (\"car\" appears only in the positive category)"
      ],
      "metadata": {
        "id": "7rco0gUfJTsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets look into how we are representing our documents\n",
        "\n"
      ],
      "metadata": {
        "id": "XpRX2oHkJhRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Function to show the feature weights of a document (to be explained later)\n",
        "def feature_values(doc, representer):\n",
        "    doc_representation = representer.transform([doc])\n",
        "    features = representer.get_feature_names()\n",
        "    return [(features[index], doc_representation[0, index]) for index in doc_representation.nonzero()[1]]\n",
        "\n",
        "pprint([feature_values(doc, vectorizer) for doc in test_data])"
      ],
      "metadata": {
        "id": "Y8EuXpoVJist"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets try again, with stop-word removal this time**"
      ],
      "metadata": {
        "id": "ITiaQF1XJx0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load the list of (english) stop-words from nltk\n",
        "stop_words = stopwords.words(\"english\")\n",
        "\n",
        "# Represent, train, predict\n",
        "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
        "vectorised_test_data = vectorizer.transform(test_data)\n",
        "classifier = LinearSVC()\n",
        "classifier.fit(vectorised_train_data, train_labels)\n",
        "\n",
        "print(classifier.predict(vectorised_test_data))\n",
        "# Expected: [Sports, Non Sports, Sports]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "pBqlkEhmJ1Ri",
        "outputId": "0e0c95b2-5f06-484d-eeb4-fbc51e221f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py:185: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  array = numpy.asarray(array, order=order, dtype=dtype)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c44b626e5bb5>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvectorised_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorised_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorised_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         )\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mTarget\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     if y_type not in [\n\u001b[1;32m    212\u001b[0m         \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         ):\n\u001b[0;32m--> 345\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    346\u001b[0m                 \u001b[0;34m\"You appear to be using a legacy multi-label data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;34m\" representation. Sequence of sequences are no\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2-Multi-Class classification problem**"
      ],
      "metadata": {
        "id": "NR6yeieEoKha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2.1-Support Vector Machine (SVM)**"
      ],
      "metadata": {
        "id": "ll-uyOBlrB8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will address the multi-class problem of detecting the language of a sentence based on 3 mutually exclusive languages (e.g., Spanish, English and French). For the sake of this example, we assume those are the only 3 languages that the documents can have. As before, we will create an artificial (and very small collection) with similar steps"
      ],
      "metadata": {
        "id": "b9L3IGcHobW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC"
      ],
      "metadata": {
        "id": "l1VVzPISowe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Artificial (and small) dataset. Spanish,English,French texts\n",
        "train_data = ['PyCon es una gran conferencia', 'Aprendizaje automatico esta listo para dominar el mundo dentro de poco',\n",
        "             'This is a great conference with a lot of amazing talks', 'AI will dominate the world in the near future',\n",
        "             'Dix chiffres por resumer le feuilleton de la loi travail']\n",
        "train_labels = [\"SP\", \"SP\", \"EN\", \"EN\", \"FR\"]\n",
        "\n",
        "test_data = ['Estoy preparandome para dominar las olimpiadas', 'Me gustaria mucho aprender el lenguage de programacion Scala',\n",
        "             'Machine Learning is amazing','Hola a todos']\n",
        "test_labels = [\"SP\", \"SP\", \"EN\", \"SP\"]"
      ],
      "metadata": {
        "id": "5rKrmOGCofBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Represent\n",
        "vectorizer = TfidfVectorizer() # Note, we are not doing stop-word removal. Stop words could be beneficial in this problems\n",
        "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
        "vectorised_test_data = vectorizer.transform(test_data)\n"
      ],
      "metadata": {
        "id": "snORR13fooXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "classifier = LinearSVC()\n",
        "classifier.fit(vectorised_train_data, train_labels)"
      ],
      "metadata": {
        "id": "Idwy7PfMo3tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "BAKDwkHJpE22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict\n",
        "predictions = classifier.predict(vectorised_test_data)\n",
        "pprint(predictions)"
      ],
      "metadata": {
        "id": "XyuaXY_xo8YG",
        "outputId": "dfdb5f11-7ed7-47af-ce78-b9bbc5dbabdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array(['SP', 'SP', 'EN', 'EN'], dtype='<U2')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last case is wrong. Can you guess why?\n",
        "\n",
        "Can we learn from never seen cases?\n"
      ],
      "metadata": {
        "id": "u3hXE4erpNoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will address the multi-label problem of labeling documents as being relevant to Sports or Politics. As before, we will create an artificial (and very small collection) with initial similar steps."
      ],
      "metadata": {
        "id": "PQ4qU73NpacM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two modifications for our example to run in a multi-label way:\n",
        "\n",
        "- Change the representation of the data viewing every document as a list of bits, representing being or not to each category. (MultiLabelBinarizer)\n",
        "\n",
        "- Run the classifier N times, once for each category where the negative cases will be the documents in all the other categories. (OneVsRestClassifier)"
      ],
      "metadata": {
        "id": "mF3nPNfapgOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier"
      ],
      "metadata": {
        "id": "D7ZiOTsYpfdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Artificial (and small) dataset. Sports and Politics\n",
        "train_data = ['Football: a great sport', 'The referee has been very bad this season', 'Our team scored 5 goals', 'I love tennis',\n",
        "              'Politics is in decline in the UK', 'Brexit means Brexit', 'The parlament wants to create new legislation',\n",
        "              'I so want to travel the world',\n",
        "              'The goverment will increase the budget for sports in the UK after the victories in the Olimpic Games',\n",
        "              \"O'Reilly has a great conference this year\"]\n",
        "train_labels = [[\"Sports\"], [\"Sports\"], [\"Sports\"], [\"Sports\"],[\"Politics\"],[\"Politics\"],[\"Politics\"],[],[\"Politics\", \"Sports\"],[]]\n",
        "\n",
        "test_data = ['Swimming is a great sport',\n",
        "             'A lot of policy changes will happen after Brexit',\n",
        "             'The table tennis team will travel to the UK soon for the European Championship',\n",
        "             'The goverment will increase the budget for sports in the UK after the victories in the Olimpic Games',\n",
        "             'PyCon is my favourite conference']\n",
        "test_labels = [[\"Sports\"], [\"Politics\"], [\"Sports\"], [\"Politics\",\"Sports\"],[]]"
      ],
      "metadata": {
        "id": "NS0lNTrAp4mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the representation of our data as a list of bit lists\n",
        "mlb = MultiLabelBinarizer()\n",
        "binary_train_labels = mlb.fit_transform(train_labels)\n",
        "binary_test_labels = mlb.transform(test_labels)\n",
        "\n",
        "print(binary_train_labels)"
      ],
      "metadata": {
        "id": "jaXXeZcrp8i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "wwamWJI3qXC7",
        "outputId": "f7a8b148-7d60-489f-f24e-fc6e28e6d2ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load the list of (english) stop-words from nltk\n",
        "stop_words = stopwords.words(\"english\")"
      ],
      "metadata": {
        "id": "PFJlx-Fzqm8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Represent\n",
        "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
        "vectorised_test_data = vectorizer.transform(test_data)\n"
      ],
      "metadata": {
        "id": "j9uL7LM_qHpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One classifer built per category using a one vs the rest approach\n",
        "classifier = OneVsRestClassifier(LinearSVC())\n",
        "classifier.fit(vectorised_train_data, binary_train_labels)"
      ],
      "metadata": {
        "id": "2hNu-wxBqrpl",
        "outputId": "19dcaa5d-809d-4f1b-b35f-d7841edfd421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=LinearSVC())"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneVsRestClassifier(estimator=LinearSVC())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=LinearSVC())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict\n",
        "predictions = classifier.predict(vectorised_test_data)"
      ],
      "metadata": {
        "id": "xv7xok4wqv0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions)\n",
        "print()"
      ],
      "metadata": {
        "id": "5tlpTAzoqzuc",
        "outputId": "3f1b72db-ad85-4110-c460-a3c9dd79386b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 1]\n",
            " [0 0]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mlb.inverse_transform(predictions))\n"
      ],
      "metadata": {
        "id": "gnCG6w7Gq35y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2.2-Naive Bayes classifie**"
      ],
      "metadata": {
        "id": "3wwLtRxYVwRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "rBX8XhTXVmQC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')"
      ],
      "metadata": {
        "id": "IVjyi0M9Vtve"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_test"
      ],
      "metadata": {
        "id": "m3bUCLrWWVhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the texts into TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "X_test = vectorizer.transform(newsgroups_test.data)\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target"
      ],
      "metadata": {
        "id": "Fg0ecCIoWD-I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Multinomial Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "y2xZG1OvWRoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the newsgroup of the test texts\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "YRr1zyKZWmz3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the classifier's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87P7RL51Wq7m",
        "outputId": "8bd2e174-d36d-43ee-9297-29359133bbbc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 77.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will load the 20 Newsgroups dataset and split it into training and test sets. Then it will transform the texts into numerical representation using TfidfVectorizer and train a Multinomial Naive Bayes classifier using the training set. Finally, it will use the trained classifier to predict the newsgroup of the test texts and evaluate the classifier’s accuracy."
      ],
      "metadata": {
        "id": "-pWH5AQDWytG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2-Sentiment Analysis**\n"
      ],
      "metadata": {
        "id": "GLdTZyjrW7XV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load the list of (english) stop-words from nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbDW9rZ5XWwd",
        "outputId": "ffe8ba32-c5dd-40e7-cba4-64964e6a3cdf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoC6WDHDXk4-",
        "outputId": "02519655-2665-4708-bc31-b4c949ea54b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Define text to be analyzed\n",
        "text = \"I love this product, it's amazing\"\n",
        "\n",
        "# Get sentiment score\n",
        "sentiment_score = sia.polarity_scores(text)\n",
        "\n",
        "# Print sentiment score\n",
        "print(sentiment_score)\n",
        "\n",
        "# Classify sentiment as positive, negative, or neutral\n",
        "if sentiment_score['compound'] > 0.5:\n",
        "    print(\"Positive sentiment\")\n",
        "elif sentiment_score['compound'] < -0.5:\n",
        "    print(\"Negative sentiment\")\n",
        "else:\n",
        "    print(\"Neutral sentiment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jxFLr14XKHM",
        "outputId": "127b5528-163c-4f60-d49e-7cf42344c83b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.273, 'pos': 0.727, 'compound': 0.8402}\n",
            "Positive sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2-Machine Translation**"
      ],
      "metadata": {
        "id": "AZ_VY1HGYuS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  http://download.pytorch.org/whl/cu80/torch-0.1.9.post2-cp27-none-linux_x86_64.whl\n"
      ],
      "metadata": {
        "id": "vVIycwWVZBTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opennmt import tokenizers\n",
        "from opennmt import models\n",
        "import torch\n",
        "\n",
        "# Tokenize the source and target text.\n",
        "source_tokenizer = tokenizers.new(\"text\", \"en\")\n",
        "source_text = \"Hello, how are you?\"\n",
        "source_tokens = source_tokenizer.tokenize(source_text)\n",
        "\n",
        "target_tokenizer = tokenizers.new(\"text\", \"fr\")\n",
        "target_text = \"Bonjour, comment vas-tu?\"\n",
        "target_tokens = target_tokenizer.tokenize(target_text)\n",
        "\n",
        "# Build the translation model.\n",
        "model = models.Transformer(\n",
        "    source_vocab_size=len(source_tokenizer.vocab),\n",
        "    target_vocab_size=len(target_tokenizer.vocab),\n",
        "    num_layers=6,\n",
        "    hidden_size=512,\n",
        "    dropout=0.1,\n",
        "    attention_dropout=0.1,\n",
        "    relu_dropout=0.1)\n",
        "model.eval()\n",
        "\n",
        "# Convert the tokens to a tensor.\n",
        "source_tokens = torch.tensor(source_tokenizer.encode(source_text)).unsqueeze(0)\n",
        "\n",
        "# Generate a translation.\n",
        "with torch.no_grad():\n",
        "    log_probs, _, _ = model(source_tokens, None, None)\n",
        "    tokens = log_probs.argmax(-1)\n",
        "\n",
        "# Decode the translation.\n",
        "translation = target_tokenizer.decode(tokens[0])\n",
        "print(translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "LwflF-UqY71M",
        "outputId": "4a274bc0-466b-4523-96ce-ccc33b9b3987"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-25d60dfa4ff1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopennmt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopennmt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Tokenize the source and target text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'opennmt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**"
      ],
      "metadata": {
        "id": "9GBsODuyD82C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1-nlp-tutorial](https://github.com/bonzanini/nlp-tutorial/blob/master/notebooks/00%20Environment%20Test.ipynb)"
      ],
      "metadata": {
        "id": "KYNVMVp-FZDf"
      }
    }
  ]
}