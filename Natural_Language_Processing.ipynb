{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyORUV6Z8cQPgTFd4SzF901k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Natural-language-processing/blob/main/Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF6zW3gnDvfA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "W_om6ncoEdax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Portion is intended to test all the dependencies that are needed for the rest of the notebooks to run properly. If everything is tested correctly you should see a \"You are all set\" message when running the code below"
      ],
      "metadata": {
        "id": "IrdA4T7iEi1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import ngrams\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from pprint import pprint\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from nltk.corpus import reuters\n",
        "from operator import itemgetter"
      ],
      "metadata": {
        "id": "7WYqdtPVEueW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHrl36fSE72Z",
        "outputId": "7de6accf-dc13-404d-ccdb-bff5e9b4af07"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test that verifies that the imports don't fail and that we have the correct files from nltk downloaded\n",
        "reuters.fileids()\n",
        "stopwords.words('english')\n",
        "word_tokenize('This is just a test')\n",
        "\n",
        "print('You are all set')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp7awaDVE25Y",
        "outputId": "25536eb0-879b-46c1-a99a-e17763bd6e9a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are all set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1- Text Classification**"
      ],
      "metadata": {
        "id": "tpEx53qfD2Mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1-Binary text classification**"
      ],
      "metadata": {
        "id": "S-BPs6llHZgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will address the binary problem of detecting Sport related documents vs any other type of documents. In order to do this we will create an artificial (and very small collection).\n",
        "\n",
        "Define a set of labelled documents that will be our training dataset. These are the documents the classifier will learn from in order to categorise future unseen documents\n",
        "\n",
        "Define a set of labelled documents that will be our testing dataset. These will be the \"unseen\" documents that the classifier will predict (without having being trained with them)\n",
        "\n",
        "Represent our training and testing documents\n",
        "\n",
        "Train the classifier based on the training data\n",
        "\n",
        "Predict the labels for the testing documents"
      ],
      "metadata": {
        "id": "Pz1m1RJUHg3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.1.1-Support Vector Machine (SVM)**"
      ],
      "metadata": {
        "id": "JJXnHE1aHxP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC"
      ],
      "metadata": {
        "id": "z4f61c9bHapn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test data. Both the full documents and their labels (\"Sports\" vs \"Non Sports\")\n",
        "train_data = ['Football: a great sport', 'The referee has been very bad this season', 'Our team scored 5 goals', 'I love tennis',\n",
        "              'Politics is in decline in the UK', 'Brexit means Brexit', 'The parlament wants to create new legislation',\n",
        "              'I so want to travel the world']\n",
        "train_labels = [\"Sports\",\"Sports\",\"Sports\",\"Sports\", \"Non Sports\", \"Non Sports\", \"Non Sports\", \"Non Sports\"]\n",
        "\n",
        "test_data = ['Swimming is a great sport',\n",
        "             'A lot of policy changes will happen after Brexit',\n",
        "             'The table tennis team will travel to the UK soon for the European Championship']\n",
        "test_labels = [\"Sports\",\"Non Sports\",\"Sports\"]"
      ],
      "metadata": {
        "id": "XN7EkSTNH_pl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Representation of the data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
        "vectorised_test_data = vectorizer.transform(test_data)\n",
        "\n",
        "# Train the classifier given the training data\n",
        "classifier = LinearSVC()\n",
        "classifier.fit(vectorised_train_data, train_labels)\n",
        "\n",
        "# Predict the labels for the test documents (not used for training)\n",
        "print(classifier.predict(vectorised_test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEsBpwm-IHQk",
        "outputId": "8ee3813e-ce7b-49a0-af2a-2611df24395a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sports' 'Non Sports' 'Non Sports']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, the third case is wrongly classified. Why do you think that might be?\n",
        "\n",
        "Matching problems (e.g., \"car\" is different than \"Cars\")\n",
        "\n",
        "Cases never seen before (e.g., the classifier has never seen the word \"table\")\n",
        "\n",
        "\"Spurious\" correlations and bias (\"car\" appears only in the positive category)"
      ],
      "metadata": {
        "id": "7rco0gUfJTsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets look into how we are representing our documents\n",
        "\n"
      ],
      "metadata": {
        "id": "XpRX2oHkJhRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Function to show the feature weights of a document (to be explained later)\n",
        "def feature_values(doc, representer):\n",
        "    doc_representation = representer.transform([doc])\n",
        "    features = representer.get_feature_names()\n",
        "    return [(features[index], doc_representation[0, index]) for index in doc_representation.nonzero()[1]]\n",
        "\n",
        "pprint([feature_values(doc, vectorizer) for doc in test_data])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "Y8EuXpoVJist",
        "outputId": "5d2b1c99-4f3a-4d91-abe2-1754063c53b3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-942df9b84944>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_representation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_representation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-942df9b84944>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_representation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_representation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-942df9b84944>\u001b[0m in \u001b[0;36mfeature_values\u001b[0;34m(doc, representer)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeature_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresenter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdoc_representation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepresenter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepresenter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_representation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_representation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets try again, with stop-word removal this time**"
      ],
      "metadata": {
        "id": "ITiaQF1XJx0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load the list of (english) stop-words from nltk\n",
        "stop_words = stopwords.words(\"english\")\n",
        "\n",
        "# Represent, train, predict\n",
        "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
        "vectorised_test_data = vectorizer.transform(test_data)\n",
        "classifier = LinearSVC()\n",
        "classifier.fit(vectorised_train_data, train_labels)\n",
        "\n",
        "print(classifier.predict(vectorised_test_data))\n",
        "# Expected: [Sports, Non Sports, Sports]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBqlkEhmJ1Ri",
        "outputId": "351480f5-109f-4d69-91ee-dd0495cb11b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sports' 'Non Sports' 'Sports']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**"
      ],
      "metadata": {
        "id": "9GBsODuyD82C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1-nlp-tutorial](https://github.com/bonzanini/nlp-tutorial/blob/master/notebooks/00%20Environment%20Test.ipynb)"
      ],
      "metadata": {
        "id": "KYNVMVp-FZDf"
      }
    }
  ]
}