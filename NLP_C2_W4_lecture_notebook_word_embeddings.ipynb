{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dr-mushtaq/Natural-language-processing/blob/main/NLP_C2_W4_lecture_notebook_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_Djbcfo1Zfr"
      },
      "source": [
        "# Word Embeddings: Hands On\n",
        "\n",
        "In previous lecture notebooks you saw all the steps needed to train the CBOW model. This notebook will walk you through how to extract the word embedding vectors from a model.\n",
        "\n",
        "Let's dive into it!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import linalg\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    # sigmoid function\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "\n",
        "def get_idx(words, word2Ind):\n",
        "    idx = []\n",
        "    for word in words:\n",
        "        idx = idx + [word2Ind[word]]\n",
        "    return idx\n",
        "\n",
        "\n",
        "def pack_idx_with_frequency(context_words, word2Ind):\n",
        "    freq_dict = defaultdict(int)\n",
        "    for word in context_words:\n",
        "        freq_dict[word] += 1\n",
        "    idxs = get_idx(context_words, word2Ind)\n",
        "    packed = []\n",
        "    for i in range(len(idxs)):\n",
        "        idx = idxs[i]\n",
        "        freq = freq_dict[context_words[i]]\n",
        "        packed.append((idx, freq))\n",
        "    return packed\n",
        "\n",
        "\n",
        "def get_vectors(data, word2Ind, V, C):\n",
        "    i = C\n",
        "    while True:\n",
        "        y = np.zeros(V)\n",
        "        x = np.zeros(V)\n",
        "        center_word = data[i]\n",
        "        y[word2Ind[center_word]] = 1\n",
        "        context_words = data[(i - C):i] + data[(i+1):(i+C+1)]\n",
        "        num_ctx_words = len(context_words)\n",
        "        for idx, freq in pack_idx_with_frequency(context_words, word2Ind):\n",
        "            x[idx] = freq/num_ctx_words\n",
        "        yield x, y\n",
        "        i += 1\n",
        "        if i >= len(data):\n",
        "            print('i is being set to 0')\n",
        "            i = 0\n",
        "\n",
        "\n",
        "def get_batches(data, word2Ind, V, C, batch_size):\n",
        "    batch_x = []\n",
        "    batch_y = []\n",
        "    for x, y in get_vectors(data, word2Ind, V, C):\n",
        "        while len(batch_x) < batch_size:\n",
        "            batch_x.append(x)\n",
        "            batch_y.append(y)\n",
        "        else:\n",
        "            yield np.array(batch_x).T, np.array(batch_y).T\n",
        "            batch = []\n",
        "\n",
        "\n",
        "def compute_pca(data, n_components=2):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        data: of dimension (m,n) where each row corresponds to a word vector\n",
        "        n_components: Number of components you want to keep.\n",
        "    Output:\n",
        "        X_reduced: data transformed in 2 dims/columns + regenerated original data\n",
        "    pass in: data as 2D NumPy array\n",
        "    \"\"\"\n",
        "\n",
        "    m, n = data.shape\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # mean center the data\n",
        "    data -= data.mean(axis=0)\n",
        "    # calculate the covariance matrix\n",
        "    R = np.cov(data, rowvar=False)\n",
        "    # calculate eigenvectors & eigenvalues of the covariance matrix\n",
        "    # use 'eigh' rather than 'eig' since R is symmetric,\n",
        "    # the performance gain is substantial\n",
        "    evals, evecs = linalg.eigh(R)\n",
        "    # sort eigenvalue in decreasing order\n",
        "    # this returns the corresponding indices of evals and evecs\n",
        "    idx = np.argsort(evals)[::-1]\n",
        "\n",
        "    evecs = evecs[:, idx]\n",
        "    # sort eigenvectors according to same index\n",
        "    evals = evals[idx]\n",
        "    # select the first n eigenvectors (n is desired dimension\n",
        "    # of rescaled data array, or dims_rescaled_data)\n",
        "    evecs = evecs[:, :n_components]\n",
        "    ### END CODE HERE ###\n",
        "    return np.dot(evecs.T, data.T).T\n",
        "\n",
        "\n",
        "def get_dict(data):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        K: the number of negative samples\n",
        "        data: the data you want to pull from\n",
        "        indices: a list of word indices\n",
        "    Output:\n",
        "        word_dict: a dictionary with the weighted probabilities of each word\n",
        "        word2Ind: returns dictionary mapping the word to its index\n",
        "        Ind2Word: returns dictionary mapping the index to its word\n",
        "    \"\"\"\n",
        "    #\n",
        "#     words = nltk.word_tokenize(data)\n",
        "    words = sorted(list(set(data)))\n",
        "    n = len(words)\n",
        "    idx = 0\n",
        "    # return these correctly\n",
        "    word2Ind = {}\n",
        "    Ind2word = {}\n",
        "    for k in words:\n",
        "        word2Ind[k] = idx\n",
        "        Ind2word[idx] = k\n",
        "        idx += 1\n",
        "    return word2Ind, Ind2word"
      ],
      "metadata": {
        "id": "puuA4gsV2q6W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a-D9_KzL1Zfu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#from utils2 import get_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhHnAiKc1Zfv"
      },
      "source": [
        "Before moving on, you will be provided with some variables needed for further procedures, which should be familiar by now. Also a trained CBOW model will be simulated, the corresponding weights and biases are provided:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ooU_4d2h1Zfw"
      },
      "outputs": [],
      "source": [
        "# Define the tokenized version of the corpus\n",
        "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
        "\n",
        "# Define V. Remember this is the size of the vocabulary\n",
        "V = 5\n",
        "\n",
        "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
        "word2Ind, Ind2word = get_dict(words)\n",
        "\n",
        "\n",
        "# Define first matrix of weights\n",
        "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
        "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
        "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
        "\n",
        "# Define second matrix of weights\n",
        "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
        "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
        "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
        "               [ 0.07055222, -0.02015138,  0.36107434],\n",
        "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
        "\n",
        "# Define first vector of biases\n",
        "b1 = np.array([[ 0.09688219],\n",
        "               [ 0.29239497],\n",
        "               [-0.27364426]])\n",
        "\n",
        "# Define second vector of biases\n",
        "b2 = np.array([[ 0.0352008 ],\n",
        "               [-0.36393384],\n",
        "               [-0.12775555],\n",
        "               [-0.34802326],\n",
        "               [-0.07017815]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbPv0aCP1Zfw"
      },
      "source": [
        "\n",
        "\n",
        "## Extracting word embedding vectors\n",
        "\n",
        "Once you have finished training the neural network, you have three options to get word embedding vectors for the words of your vocabulary, based on the weight matrices $\\mathbf{W_1}$ and/or $\\mathbf{W_2}$.\n",
        "\n",
        "### Option 1: extract embedding vectors from $\\mathbf{W_1}$\n",
        "\n",
        "The first option is to take the columns of $\\mathbf{W_1}$ as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.\n",
        "\n",
        "> Note: in this practice notebooks the values of the word embedding vectors are meaningless after a single iteration with just one training example, but here's how you would proceed after the training process is complete.\n",
        "\n",
        "For example $\\mathbf{W_1}$ is this matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVrjav3x1Zfx",
        "outputId": "efc8987b-d829-49ad-c958-e43c9d890f79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
              "       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
              "       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Print W1\n",
        "W1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4X8QRD61Zfy"
      },
      "source": [
        "The first column, which is a 3-element vector, is the embedding vector of the first word of your vocabulary. The second column is the word embedding vector for the second word, and so on.\n",
        "\n",
        "The first, second, etc. words are ordered as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXbSnj1f1Zfy",
        "outputId": "48268e58-62fd-4853-edb2-3cb48a618e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am\n",
            "because\n",
            "happy\n",
            "i\n",
            "learning\n"
          ]
        }
      ],
      "source": [
        "# Print corresponding word for each index within vocabulary's range\n",
        "for i in range(V):\n",
        "    print(Ind2word[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9g6YvDE1Zfz"
      },
      "source": [
        "So the word embedding vectors corresponding to each word are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPln_Sy71Zfz",
        "outputId": "5e86b405-b5d3-4157-f19d-843670b4800d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am: [0.41687358 0.32735501 0.26637602]\n",
            "because: [ 0.08854191  0.22795148 -0.23846886]\n",
            "happy: [-0.23495225 -0.23951958 -0.37770863]\n",
            "i: [ 0.28320538  0.4117634  -0.11399446]\n",
            "learning: [ 0.41800106 -0.23924344  0.34008124]\n"
          ]
        }
      ],
      "source": [
        "# Loop through each word of the vocabulary\n",
        "for word in word2Ind:\n",
        "    # Extract the column corresponding to the index of the word in the vocabulary\n",
        "    word_embedding_vector = W1[:, word2Ind[word]]\n",
        "    # Print word alongside word embedding vector\n",
        "    print(f'{word}: {word_embedding_vector}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxQ3K85n1Zf0"
      },
      "source": [
        "### Option 2: extract embedding vectors from $\\mathbf{W_2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81thRK6t1Zf0"
      },
      "source": [
        "The second option is to take $\\mathbf{W_2}$ transposed, and take its columns as the word embedding vectors just like you did for $\\mathbf{W_1}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjvHTmlH1Zf1",
        "outputId": "c99a6ee8-83b8-4a59-b583-6e1648c9eae9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n",
              "       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n",
              "       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Print transposed W2\n",
        "W2.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zH6-_S0j1Zf1",
        "outputId": "a6434775-ee21-432d-9a31-66d151b579e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am: [-0.22182064 -0.43008631  0.13310965]\n",
            "because: [0.08476603 0.08123194 0.1772054 ]\n",
            "happy: [ 0.1871551  -0.06107263 -0.1790735 ]\n",
            "i: [ 0.07055222 -0.02015138  0.36107434]\n",
            "learning: [ 0.33480474 -0.39423389 -0.43959196]\n"
          ]
        }
      ],
      "source": [
        "# Loop through each word of the vocabulary\n",
        "for word in word2Ind:\n",
        "    # Extract the column corresponding to the index of the word in the vocabulary\n",
        "    word_embedding_vector = W2.T[:, word2Ind[word]]\n",
        "    # Print word alongside word embedding vector\n",
        "    print(f'{word}: {word_embedding_vector}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-ZRY6nE1Zf1"
      },
      "source": [
        "### Option 3: extract embedding vectors from $\\mathbf{W_1}$ and $\\mathbf{W_2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7CFAop91Zf1"
      },
      "source": [
        "The third option, which is the one you will use in this week's assignment, uses the average of $\\mathbf{W_1}$ and $\\mathbf{W_2^\\top}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN67zJuD1Zf1"
      },
      "source": [
        "**Calculate the average of $\\mathbf{W_1}$ and $\\mathbf{W_2^\\top}$, and store the result in `W3`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy_RKSzE1Zf2",
        "outputId": "8f5f33e0-557e-4c39-9f5d-ea51dc5e08b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n",
              "       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n",
              "       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Compute W3 as the average of W1 and W2 transposed\n",
        "W3 = (W1+W2.T)/2\n",
        "\n",
        "# Print W3\n",
        "W3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY7khctI1Zf2"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "    array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n",
        "           [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n",
        "           [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prnFvNXR1Zf2"
      },
      "source": [
        "Extracting the word embedding vectors works just like the two previous options, by taking the columns of the matrix you've just created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLovXy6l1Zf2",
        "outputId": "7c52edd9-7f99-4e74-ef9e-1e0c032d89ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am: [ 0.09752647 -0.05136565  0.19974284]\n",
            "because: [ 0.08665397  0.15459171 -0.03063173]\n",
            "happy: [-0.02389858 -0.15029611 -0.27839106]\n",
            "i: [0.1768788  0.19580601 0.12353994]\n",
            "learning: [ 0.3764029  -0.31673866 -0.04975536]\n"
          ]
        }
      ],
      "source": [
        "# Loop through each word of the vocabulary\n",
        "for word in word2Ind:\n",
        "    # Extract the column corresponding to the index of the word in the vocabulary\n",
        "    word_embedding_vector = W3[:, word2Ind[word]]\n",
        "    # Print word alongside word embedding vector\n",
        "    print(f'{word}: {word_embedding_vector}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zn2cqc51Zf2"
      },
      "source": [
        "Now you know 3 different options to get the word embedding vectors from a model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utVelPwu1Zf3"
      },
      "source": [
        "### How this practice relates to and differs from the upcoming graded assignment\n",
        "\n",
        "- After extracting the word embedding vectors, you will use principal component analysis (PCA) to visualize the vectors, which will enable you to perform an intrinsic evaluation of the quality of the vectors, as explained in the lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgCRe4321Zf3"
      },
      "source": [
        "**Congratulations on finishing all lecture notebooks for this week!**\n",
        "\n",
        "You're now ready to take on this week's assignment!\n",
        "\n",
        "**Keep it up!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}